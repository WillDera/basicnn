{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0L7ECj8cZ5Mri4hLJM9ad",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WillDera/simplenn/blob/master/Basic_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdBselDJZp2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD \n",
        "from keras.utils import np_utils\n",
        "np.random.seed(1671) #for reproductivity"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pazFQjBvd04m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# network and training\n",
        "NB_EPOCH = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10 # no. of outputs = no. of digits\n",
        "OPTIMIZER = SGD()\n",
        "N_HIDDEN = 128 \n",
        "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
        "DROPOUT = 0.3"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-ZIVAcNey0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data: shuffled and split b/w train and test sets\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# x_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784 \n",
        "RESHAPED = 784"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhEAaiqnfWyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(60000, RESHAPED)\n",
        "x_test = x_test.reshape(10000, RESHAPED)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tz5kJbPi51i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9f288994-8f3f-44a8-b230-bbd1c33a0ad8"
      },
      "source": [
        "# normalize \n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVcWXJ8pjJRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert class vectors to binary class matrices\n",
        "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHangYXajbaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "851ab045-1915-4073-b7a4-ca7058c08904"
      },
      "source": [
        "#10 outputs\n",
        "# final stage -> softmax\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
        "\n",
        "# adding a hidden layer and an activation function 'relu', to imporve our simple neural net.\n",
        "model.add(Activation('relu'))\n",
        "# adding the dropout regularization in order to try improving our Neural net.\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(N_HIDDEN))\n",
        "model.add(Activation('relu'))\n",
        "# another dropout regularizer\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(NB_CLASSES))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               1408      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 10,548\n",
            "Trainable params: 10,548\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58hxGLl4kDP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compiling our model\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzYjYj1RkXR8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56e9de7a-7d00-4c33-d448-e4b37eb75ea8"
      },
      "source": [
        "# training our model\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/200\n",
            "48000/48000 [==============================] - 1s 22us/step - loss: 2.1666 - accuracy: 0.2101 - val_loss: 1.8910 - val_accuracy: 0.4140\n",
            "Epoch 2/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 1.7411 - accuracy: 0.3954 - val_loss: 1.2528 - val_accuracy: 0.7338\n",
            "Epoch 3/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 1.3951 - accuracy: 0.5106 - val_loss: 0.9095 - val_accuracy: 0.8104\n",
            "Epoch 4/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 1.2305 - accuracy: 0.5660 - val_loss: 0.7619 - val_accuracy: 0.8336\n",
            "Epoch 5/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 1.1390 - accuracy: 0.5999 - val_loss: 0.6811 - val_accuracy: 0.8458\n",
            "Epoch 6/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 1.0822 - accuracy: 0.6210 - val_loss: 0.6315 - val_accuracy: 0.8565\n",
            "Epoch 7/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 1.0428 - accuracy: 0.6335 - val_loss: 0.5933 - val_accuracy: 0.8615\n",
            "Epoch 8/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.9973 - accuracy: 0.6515 - val_loss: 0.5626 - val_accuracy: 0.8683\n",
            "Epoch 9/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.9793 - accuracy: 0.6548 - val_loss: 0.5425 - val_accuracy: 0.8686\n",
            "Epoch 10/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.9551 - accuracy: 0.6658 - val_loss: 0.5249 - val_accuracy: 0.8719\n",
            "Epoch 11/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.9305 - accuracy: 0.6732 - val_loss: 0.5086 - val_accuracy: 0.8770\n",
            "Epoch 12/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.9095 - accuracy: 0.6809 - val_loss: 0.4940 - val_accuracy: 0.8790\n",
            "Epoch 13/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.9008 - accuracy: 0.6847 - val_loss: 0.4848 - val_accuracy: 0.8796\n",
            "Epoch 14/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.8816 - accuracy: 0.6911 - val_loss: 0.4735 - val_accuracy: 0.8829\n",
            "Epoch 15/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.8683 - accuracy: 0.6953 - val_loss: 0.4637 - val_accuracy: 0.8842\n",
            "Epoch 16/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.8607 - accuracy: 0.7020 - val_loss: 0.4562 - val_accuracy: 0.8854\n",
            "Epoch 17/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.8459 - accuracy: 0.7026 - val_loss: 0.4515 - val_accuracy: 0.8854\n",
            "Epoch 18/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.8355 - accuracy: 0.7116 - val_loss: 0.4426 - val_accuracy: 0.8875\n",
            "Epoch 19/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.8288 - accuracy: 0.7114 - val_loss: 0.4398 - val_accuracy: 0.8888\n",
            "Epoch 20/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.8335 - accuracy: 0.7096 - val_loss: 0.4392 - val_accuracy: 0.8892\n",
            "Epoch 21/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.8171 - accuracy: 0.7131 - val_loss: 0.4344 - val_accuracy: 0.8900\n",
            "Epoch 22/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.8154 - accuracy: 0.7176 - val_loss: 0.4286 - val_accuracy: 0.8919\n",
            "Epoch 23/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.8050 - accuracy: 0.7193 - val_loss: 0.4222 - val_accuracy: 0.8933\n",
            "Epoch 24/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.8009 - accuracy: 0.7228 - val_loss: 0.4204 - val_accuracy: 0.8930\n",
            "Epoch 25/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.7916 - accuracy: 0.7266 - val_loss: 0.4193 - val_accuracy: 0.8930\n",
            "Epoch 26/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7917 - accuracy: 0.7254 - val_loss: 0.4184 - val_accuracy: 0.8932\n",
            "Epoch 27/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7904 - accuracy: 0.7260 - val_loss: 0.4142 - val_accuracy: 0.8947\n",
            "Epoch 28/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7810 - accuracy: 0.7315 - val_loss: 0.4140 - val_accuracy: 0.8940\n",
            "Epoch 29/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.7723 - accuracy: 0.7331 - val_loss: 0.4133 - val_accuracy: 0.8943\n",
            "Epoch 30/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7663 - accuracy: 0.7341 - val_loss: 0.4092 - val_accuracy: 0.8947\n",
            "Epoch 31/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7643 - accuracy: 0.7344 - val_loss: 0.4021 - val_accuracy: 0.8953\n",
            "Epoch 32/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7625 - accuracy: 0.7377 - val_loss: 0.3996 - val_accuracy: 0.8953\n",
            "Epoch 33/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7614 - accuracy: 0.7394 - val_loss: 0.4011 - val_accuracy: 0.8947\n",
            "Epoch 34/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7597 - accuracy: 0.7401 - val_loss: 0.4000 - val_accuracy: 0.8953\n",
            "Epoch 35/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.7567 - accuracy: 0.7421 - val_loss: 0.3993 - val_accuracy: 0.8962\n",
            "Epoch 36/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.7505 - accuracy: 0.7420 - val_loss: 0.3980 - val_accuracy: 0.8958\n",
            "Epoch 37/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7503 - accuracy: 0.7432 - val_loss: 0.3924 - val_accuracy: 0.8967\n",
            "Epoch 38/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.7395 - accuracy: 0.7466 - val_loss: 0.3934 - val_accuracy: 0.8957\n",
            "Epoch 39/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7434 - accuracy: 0.7462 - val_loss: 0.3894 - val_accuracy: 0.8970\n",
            "Epoch 40/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.7386 - accuracy: 0.7491 - val_loss: 0.3908 - val_accuracy: 0.8963\n",
            "Epoch 41/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7405 - accuracy: 0.7481 - val_loss: 0.3892 - val_accuracy: 0.8972\n",
            "Epoch 42/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7332 - accuracy: 0.7511 - val_loss: 0.3851 - val_accuracy: 0.8972\n",
            "Epoch 43/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7343 - accuracy: 0.7490 - val_loss: 0.3831 - val_accuracy: 0.8981\n",
            "Epoch 44/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7225 - accuracy: 0.7527 - val_loss: 0.3827 - val_accuracy: 0.8982\n",
            "Epoch 45/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7290 - accuracy: 0.7514 - val_loss: 0.3792 - val_accuracy: 0.8976\n",
            "Epoch 46/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.7245 - accuracy: 0.7546 - val_loss: 0.3834 - val_accuracy: 0.8986\n",
            "Epoch 47/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7238 - accuracy: 0.7534 - val_loss: 0.3806 - val_accuracy: 0.8983\n",
            "Epoch 48/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.7283 - accuracy: 0.7510 - val_loss: 0.3776 - val_accuracy: 0.8981\n",
            "Epoch 49/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.7137 - accuracy: 0.7580 - val_loss: 0.3770 - val_accuracy: 0.8992\n",
            "Epoch 50/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7161 - accuracy: 0.7600 - val_loss: 0.3779 - val_accuracy: 0.8988\n",
            "Epoch 51/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7104 - accuracy: 0.7596 - val_loss: 0.3749 - val_accuracy: 0.8991\n",
            "Epoch 52/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.7077 - accuracy: 0.7588 - val_loss: 0.3750 - val_accuracy: 0.8997\n",
            "Epoch 53/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7064 - accuracy: 0.7616 - val_loss: 0.3706 - val_accuracy: 0.9001\n",
            "Epoch 54/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7059 - accuracy: 0.7593 - val_loss: 0.3728 - val_accuracy: 0.9013\n",
            "Epoch 55/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7063 - accuracy: 0.7592 - val_loss: 0.3725 - val_accuracy: 0.9015\n",
            "Epoch 56/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7094 - accuracy: 0.7580 - val_loss: 0.3701 - val_accuracy: 0.9019\n",
            "Epoch 57/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.7027 - accuracy: 0.7627 - val_loss: 0.3690 - val_accuracy: 0.9002\n",
            "Epoch 58/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6998 - accuracy: 0.7639 - val_loss: 0.3675 - val_accuracy: 0.9016\n",
            "Epoch 59/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6974 - accuracy: 0.7635 - val_loss: 0.3708 - val_accuracy: 0.9002\n",
            "Epoch 60/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6879 - accuracy: 0.7677 - val_loss: 0.3638 - val_accuracy: 0.9008\n",
            "Epoch 61/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6945 - accuracy: 0.7667 - val_loss: 0.3646 - val_accuracy: 0.9022\n",
            "Epoch 62/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6966 - accuracy: 0.7649 - val_loss: 0.3670 - val_accuracy: 0.9014\n",
            "Epoch 63/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6896 - accuracy: 0.7681 - val_loss: 0.3644 - val_accuracy: 0.9026\n",
            "Epoch 64/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6865 - accuracy: 0.7685 - val_loss: 0.3687 - val_accuracy: 0.9014\n",
            "Epoch 65/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6945 - accuracy: 0.7667 - val_loss: 0.3643 - val_accuracy: 0.9013\n",
            "Epoch 66/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6799 - accuracy: 0.7713 - val_loss: 0.3646 - val_accuracy: 0.9021\n",
            "Epoch 67/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6845 - accuracy: 0.7689 - val_loss: 0.3634 - val_accuracy: 0.9017\n",
            "Epoch 68/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6849 - accuracy: 0.7691 - val_loss: 0.3623 - val_accuracy: 0.9022\n",
            "Epoch 69/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6842 - accuracy: 0.7710 - val_loss: 0.3611 - val_accuracy: 0.9027\n",
            "Epoch 70/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6798 - accuracy: 0.7715 - val_loss: 0.3614 - val_accuracy: 0.9020\n",
            "Epoch 71/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6757 - accuracy: 0.7741 - val_loss: 0.3598 - val_accuracy: 0.9022\n",
            "Epoch 72/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6787 - accuracy: 0.7732 - val_loss: 0.3628 - val_accuracy: 0.9016\n",
            "Epoch 73/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6727 - accuracy: 0.7736 - val_loss: 0.3625 - val_accuracy: 0.9013\n",
            "Epoch 74/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6706 - accuracy: 0.7762 - val_loss: 0.3569 - val_accuracy: 0.9030\n",
            "Epoch 75/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6719 - accuracy: 0.7754 - val_loss: 0.3604 - val_accuracy: 0.9020\n",
            "Epoch 76/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6681 - accuracy: 0.7764 - val_loss: 0.3542 - val_accuracy: 0.9033\n",
            "Epoch 77/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6623 - accuracy: 0.7783 - val_loss: 0.3571 - val_accuracy: 0.9024\n",
            "Epoch 78/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6692 - accuracy: 0.7746 - val_loss: 0.3570 - val_accuracy: 0.9012\n",
            "Epoch 79/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6686 - accuracy: 0.7775 - val_loss: 0.3593 - val_accuracy: 0.9011\n",
            "Epoch 80/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6653 - accuracy: 0.7806 - val_loss: 0.3574 - val_accuracy: 0.9021\n",
            "Epoch 81/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6666 - accuracy: 0.7772 - val_loss: 0.3600 - val_accuracy: 0.9017\n",
            "Epoch 82/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6663 - accuracy: 0.7778 - val_loss: 0.3561 - val_accuracy: 0.9030\n",
            "Epoch 83/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6606 - accuracy: 0.7769 - val_loss: 0.3521 - val_accuracy: 0.9034\n",
            "Epoch 84/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6611 - accuracy: 0.7798 - val_loss: 0.3546 - val_accuracy: 0.9028\n",
            "Epoch 85/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6603 - accuracy: 0.7795 - val_loss: 0.3559 - val_accuracy: 0.9030\n",
            "Epoch 86/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6607 - accuracy: 0.7803 - val_loss: 0.3526 - val_accuracy: 0.9034\n",
            "Epoch 87/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6596 - accuracy: 0.7808 - val_loss: 0.3560 - val_accuracy: 0.9043\n",
            "Epoch 88/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6623 - accuracy: 0.7780 - val_loss: 0.3548 - val_accuracy: 0.9024\n",
            "Epoch 89/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6544 - accuracy: 0.7839 - val_loss: 0.3513 - val_accuracy: 0.9040\n",
            "Epoch 90/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6508 - accuracy: 0.7812 - val_loss: 0.3571 - val_accuracy: 0.9026\n",
            "Epoch 91/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6552 - accuracy: 0.7813 - val_loss: 0.3541 - val_accuracy: 0.9031\n",
            "Epoch 92/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6608 - accuracy: 0.7820 - val_loss: 0.3528 - val_accuracy: 0.9048\n",
            "Epoch 93/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6508 - accuracy: 0.7843 - val_loss: 0.3508 - val_accuracy: 0.9037\n",
            "Epoch 94/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6497 - accuracy: 0.7832 - val_loss: 0.3505 - val_accuracy: 0.9031\n",
            "Epoch 95/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6483 - accuracy: 0.7844 - val_loss: 0.3491 - val_accuracy: 0.9026\n",
            "Epoch 96/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6523 - accuracy: 0.7845 - val_loss: 0.3500 - val_accuracy: 0.9041\n",
            "Epoch 97/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6474 - accuracy: 0.7867 - val_loss: 0.3558 - val_accuracy: 0.9022\n",
            "Epoch 98/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6521 - accuracy: 0.7840 - val_loss: 0.3547 - val_accuracy: 0.9037\n",
            "Epoch 99/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6442 - accuracy: 0.7854 - val_loss: 0.3523 - val_accuracy: 0.9040\n",
            "Epoch 100/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6457 - accuracy: 0.7860 - val_loss: 0.3519 - val_accuracy: 0.9023\n",
            "Epoch 101/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6458 - accuracy: 0.7859 - val_loss: 0.3524 - val_accuracy: 0.9042\n",
            "Epoch 102/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6489 - accuracy: 0.7837 - val_loss: 0.3533 - val_accuracy: 0.9047\n",
            "Epoch 103/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6445 - accuracy: 0.7860 - val_loss: 0.3504 - val_accuracy: 0.9044\n",
            "Epoch 104/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6400 - accuracy: 0.7895 - val_loss: 0.3541 - val_accuracy: 0.9032\n",
            "Epoch 105/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6447 - accuracy: 0.7868 - val_loss: 0.3565 - val_accuracy: 0.9026\n",
            "Epoch 106/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6420 - accuracy: 0.7889 - val_loss: 0.3519 - val_accuracy: 0.9039\n",
            "Epoch 107/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6428 - accuracy: 0.7886 - val_loss: 0.3498 - val_accuracy: 0.9039\n",
            "Epoch 108/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6390 - accuracy: 0.7876 - val_loss: 0.3514 - val_accuracy: 0.9026\n",
            "Epoch 109/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6443 - accuracy: 0.7897 - val_loss: 0.3525 - val_accuracy: 0.9032\n",
            "Epoch 110/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6363 - accuracy: 0.7900 - val_loss: 0.3466 - val_accuracy: 0.9053\n",
            "Epoch 111/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6347 - accuracy: 0.7906 - val_loss: 0.3512 - val_accuracy: 0.9041\n",
            "Epoch 112/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6370 - accuracy: 0.7882 - val_loss: 0.3496 - val_accuracy: 0.9048\n",
            "Epoch 113/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6387 - accuracy: 0.7898 - val_loss: 0.3517 - val_accuracy: 0.9032\n",
            "Epoch 114/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6330 - accuracy: 0.7909 - val_loss: 0.3520 - val_accuracy: 0.9025\n",
            "Epoch 115/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6397 - accuracy: 0.7888 - val_loss: 0.3512 - val_accuracy: 0.9029\n",
            "Epoch 116/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6370 - accuracy: 0.7896 - val_loss: 0.3515 - val_accuracy: 0.9032\n",
            "Epoch 117/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6333 - accuracy: 0.7918 - val_loss: 0.3521 - val_accuracy: 0.9022\n",
            "Epoch 118/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6387 - accuracy: 0.7921 - val_loss: 0.3506 - val_accuracy: 0.9031\n",
            "Epoch 119/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6299 - accuracy: 0.7919 - val_loss: 0.3475 - val_accuracy: 0.9034\n",
            "Epoch 120/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6314 - accuracy: 0.7917 - val_loss: 0.3576 - val_accuracy: 0.9029\n",
            "Epoch 121/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6341 - accuracy: 0.7904 - val_loss: 0.3540 - val_accuracy: 0.9028\n",
            "Epoch 122/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6340 - accuracy: 0.7905 - val_loss: 0.3514 - val_accuracy: 0.9019\n",
            "Epoch 123/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6347 - accuracy: 0.7929 - val_loss: 0.3527 - val_accuracy: 0.9032\n",
            "Epoch 124/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6312 - accuracy: 0.7903 - val_loss: 0.3563 - val_accuracy: 0.9014\n",
            "Epoch 125/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6277 - accuracy: 0.7942 - val_loss: 0.3472 - val_accuracy: 0.9021\n",
            "Epoch 126/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6259 - accuracy: 0.7943 - val_loss: 0.3479 - val_accuracy: 0.9025\n",
            "Epoch 127/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6250 - accuracy: 0.7945 - val_loss: 0.3466 - val_accuracy: 0.9018\n",
            "Epoch 128/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6297 - accuracy: 0.7919 - val_loss: 0.3481 - val_accuracy: 0.9021\n",
            "Epoch 129/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6280 - accuracy: 0.7921 - val_loss: 0.3550 - val_accuracy: 0.9030\n",
            "Epoch 130/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6299 - accuracy: 0.7917 - val_loss: 0.3558 - val_accuracy: 0.9013\n",
            "Epoch 131/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6164 - accuracy: 0.7959 - val_loss: 0.3478 - val_accuracy: 0.9034\n",
            "Epoch 132/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6250 - accuracy: 0.7941 - val_loss: 0.3522 - val_accuracy: 0.9027\n",
            "Epoch 133/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6209 - accuracy: 0.7955 - val_loss: 0.3486 - val_accuracy: 0.9032\n",
            "Epoch 134/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6250 - accuracy: 0.7946 - val_loss: 0.3520 - val_accuracy: 0.9029\n",
            "Epoch 135/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6198 - accuracy: 0.7973 - val_loss: 0.3450 - val_accuracy: 0.9038\n",
            "Epoch 136/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6247 - accuracy: 0.7949 - val_loss: 0.3528 - val_accuracy: 0.9029\n",
            "Epoch 137/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6218 - accuracy: 0.7958 - val_loss: 0.3503 - val_accuracy: 0.9024\n",
            "Epoch 138/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6192 - accuracy: 0.7971 - val_loss: 0.3512 - val_accuracy: 0.9029\n",
            "Epoch 139/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6264 - accuracy: 0.7921 - val_loss: 0.3478 - val_accuracy: 0.9045\n",
            "Epoch 140/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6160 - accuracy: 0.7960 - val_loss: 0.3494 - val_accuracy: 0.9030\n",
            "Epoch 141/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6190 - accuracy: 0.7969 - val_loss: 0.3510 - val_accuracy: 0.9012\n",
            "Epoch 142/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6223 - accuracy: 0.7960 - val_loss: 0.3473 - val_accuracy: 0.9037\n",
            "Epoch 143/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6146 - accuracy: 0.7991 - val_loss: 0.3496 - val_accuracy: 0.9026\n",
            "Epoch 144/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6205 - accuracy: 0.7970 - val_loss: 0.3490 - val_accuracy: 0.9038\n",
            "Epoch 145/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6200 - accuracy: 0.7983 - val_loss: 0.3511 - val_accuracy: 0.9031\n",
            "Epoch 146/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6137 - accuracy: 0.7987 - val_loss: 0.3514 - val_accuracy: 0.9025\n",
            "Epoch 147/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6158 - accuracy: 0.7985 - val_loss: 0.3485 - val_accuracy: 0.9035\n",
            "Epoch 148/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6126 - accuracy: 0.8001 - val_loss: 0.3476 - val_accuracy: 0.9036\n",
            "Epoch 149/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6230 - accuracy: 0.7965 - val_loss: 0.3531 - val_accuracy: 0.9022\n",
            "Epoch 150/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6208 - accuracy: 0.7969 - val_loss: 0.3569 - val_accuracy: 0.9032\n",
            "Epoch 151/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6160 - accuracy: 0.7965 - val_loss: 0.3483 - val_accuracy: 0.9040\n",
            "Epoch 152/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6172 - accuracy: 0.7977 - val_loss: 0.3506 - val_accuracy: 0.9040\n",
            "Epoch 153/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6128 - accuracy: 0.7983 - val_loss: 0.3554 - val_accuracy: 0.9014\n",
            "Epoch 154/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6123 - accuracy: 0.7995 - val_loss: 0.3509 - val_accuracy: 0.9029\n",
            "Epoch 155/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6089 - accuracy: 0.7999 - val_loss: 0.3476 - val_accuracy: 0.9046\n",
            "Epoch 156/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6126 - accuracy: 0.7987 - val_loss: 0.3461 - val_accuracy: 0.9043\n",
            "Epoch 157/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6085 - accuracy: 0.7993 - val_loss: 0.3520 - val_accuracy: 0.9028\n",
            "Epoch 158/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6101 - accuracy: 0.7983 - val_loss: 0.3458 - val_accuracy: 0.9039\n",
            "Epoch 159/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6067 - accuracy: 0.7998 - val_loss: 0.3483 - val_accuracy: 0.9047\n",
            "Epoch 160/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6096 - accuracy: 0.8000 - val_loss: 0.3488 - val_accuracy: 0.9040\n",
            "Epoch 161/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6052 - accuracy: 0.8025 - val_loss: 0.3474 - val_accuracy: 0.9038\n",
            "Epoch 162/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6112 - accuracy: 0.8009 - val_loss: 0.3466 - val_accuracy: 0.9060\n",
            "Epoch 163/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6084 - accuracy: 0.8007 - val_loss: 0.3547 - val_accuracy: 0.9013\n",
            "Epoch 164/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6064 - accuracy: 0.8009 - val_loss: 0.3500 - val_accuracy: 0.9050\n",
            "Epoch 165/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6047 - accuracy: 0.8025 - val_loss: 0.3535 - val_accuracy: 0.9028\n",
            "Epoch 166/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6092 - accuracy: 0.7995 - val_loss: 0.3489 - val_accuracy: 0.9055\n",
            "Epoch 167/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6089 - accuracy: 0.7990 - val_loss: 0.3517 - val_accuracy: 0.9038\n",
            "Epoch 168/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6106 - accuracy: 0.7998 - val_loss: 0.3490 - val_accuracy: 0.9042\n",
            "Epoch 169/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.5962 - accuracy: 0.8026 - val_loss: 0.3468 - val_accuracy: 0.9052\n",
            "Epoch 170/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6007 - accuracy: 0.8044 - val_loss: 0.3415 - val_accuracy: 0.9067\n",
            "Epoch 171/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6061 - accuracy: 0.8009 - val_loss: 0.3528 - val_accuracy: 0.9037\n",
            "Epoch 172/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6065 - accuracy: 0.8010 - val_loss: 0.3549 - val_accuracy: 0.9026\n",
            "Epoch 173/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6063 - accuracy: 0.8009 - val_loss: 0.3527 - val_accuracy: 0.9043\n",
            "Epoch 174/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6016 - accuracy: 0.8042 - val_loss: 0.3493 - val_accuracy: 0.9047\n",
            "Epoch 175/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6025 - accuracy: 0.8021 - val_loss: 0.3543 - val_accuracy: 0.9024\n",
            "Epoch 176/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6032 - accuracy: 0.8049 - val_loss: 0.3503 - val_accuracy: 0.9043\n",
            "Epoch 177/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5980 - accuracy: 0.8022 - val_loss: 0.3471 - val_accuracy: 0.9044\n",
            "Epoch 178/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6009 - accuracy: 0.8023 - val_loss: 0.3502 - val_accuracy: 0.9029\n",
            "Epoch 179/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5989 - accuracy: 0.8040 - val_loss: 0.3493 - val_accuracy: 0.9027\n",
            "Epoch 180/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5983 - accuracy: 0.8033 - val_loss: 0.3515 - val_accuracy: 0.9036\n",
            "Epoch 181/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6044 - accuracy: 0.8023 - val_loss: 0.3483 - val_accuracy: 0.9050\n",
            "Epoch 182/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5955 - accuracy: 0.8051 - val_loss: 0.3481 - val_accuracy: 0.9044\n",
            "Epoch 183/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5999 - accuracy: 0.8042 - val_loss: 0.3519 - val_accuracy: 0.9036\n",
            "Epoch 184/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.5978 - accuracy: 0.8040 - val_loss: 0.3513 - val_accuracy: 0.9047\n",
            "Epoch 185/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6073 - accuracy: 0.7992 - val_loss: 0.3521 - val_accuracy: 0.9027\n",
            "Epoch 186/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5952 - accuracy: 0.8055 - val_loss: 0.3542 - val_accuracy: 0.9033\n",
            "Epoch 187/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6036 - accuracy: 0.8026 - val_loss: 0.3514 - val_accuracy: 0.9039\n",
            "Epoch 188/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5955 - accuracy: 0.8048 - val_loss: 0.3488 - val_accuracy: 0.9038\n",
            "Epoch 189/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.6017 - accuracy: 0.8037 - val_loss: 0.3548 - val_accuracy: 0.9047\n",
            "Epoch 190/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5963 - accuracy: 0.8047 - val_loss: 0.3491 - val_accuracy: 0.9048\n",
            "Epoch 191/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5971 - accuracy: 0.8041 - val_loss: 0.3528 - val_accuracy: 0.9036\n",
            "Epoch 192/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5972 - accuracy: 0.8053 - val_loss: 0.3553 - val_accuracy: 0.9038\n",
            "Epoch 193/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5997 - accuracy: 0.8028 - val_loss: 0.3498 - val_accuracy: 0.9039\n",
            "Epoch 194/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5976 - accuracy: 0.8045 - val_loss: 0.3509 - val_accuracy: 0.9043\n",
            "Epoch 195/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5934 - accuracy: 0.8071 - val_loss: 0.3551 - val_accuracy: 0.9024\n",
            "Epoch 196/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5948 - accuracy: 0.8048 - val_loss: 0.3487 - val_accuracy: 0.9052\n",
            "Epoch 197/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.6006 - accuracy: 0.8043 - val_loss: 0.3559 - val_accuracy: 0.9014\n",
            "Epoch 198/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5944 - accuracy: 0.8048 - val_loss: 0.3549 - val_accuracy: 0.9026\n",
            "Epoch 199/200\n",
            "48000/48000 [==============================] - 1s 21us/step - loss: 0.5957 - accuracy: 0.8050 - val_loss: 0.3568 - val_accuracy: 0.9023\n",
            "Epoch 200/200\n",
            "48000/48000 [==============================] - 1s 20us/step - loss: 0.5996 - accuracy: 0.8041 - val_loss: 0.3596 - val_accuracy: 0.9003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljMIqncYko9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b80f0c32-be56-4e70-9566-818e114d382a"
      },
      "source": [
        "# evaluating our model\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=VERBOSE)\n",
        "print(\"Test Score: \", score[0])\n",
        "print(\"Test Accuracy: \", score[1])\n",
        "\n",
        "# previous values without an additional hidden layer and the relu activation function\n",
        "# Test Score:  0.2773810000360012\n",
        "# Test Accuracy:  0.9225000143051147\n",
        "\n",
        "# values before trying out The Dropout \n",
        "# Test Score:  0.15192453786525875\n",
        "# Test Accuracy:  0.9555000066757202"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 22us/step\n",
            "Test Score:  0.36275843107700345\n",
            "Test Accuracy:  0.899399995803833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy2OnMBgl_H5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding the dropout to our model depleted its performance on the test, train and validation data, we can try increasing the \n",
        "# number of epochs to see if there's any form of improvement"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}